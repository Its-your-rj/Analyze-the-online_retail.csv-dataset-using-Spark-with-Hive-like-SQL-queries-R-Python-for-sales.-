# Sales_Analyzer: End-to-End Retail Analytics Pipeline üìà

This repository contains a reproducible, end-to-end data pipeline that ingests raw retail transaction data, cleans and models it with **Apache Spark**, runs analytics queries, and produces business-ready outputs like **CSVs, PNG plots, and an Excel workbook**. The entire pipeline is orchestrated with PowerShell for a one-click run on **Windows** and is designed to run completely on your local machine.

---

## üéØ Goals & Deliverables

The primary objective of this project is to provide a clean, production-style template for retail analytics using Apache Spark. It demonstrates a full data lifecycle, from a raw CSV file to actionable business artifacts, executed with a single command.

**What you get when you run the pipeline:**

* **Cleaned Parquet Dataset**: A columnar, compressed, and analytics-friendly dataset ready for BI tools or further analysis.
* **KPI Exports (CSV)**: Key metrics like monthly revenue, top customers, and top products.
* **Visualizations (PNG)**: Charts generated by both Python (Matplotlib) and R (ggplot2).
* **Business-Ready Report (Excel)**: An optional multi-sheet Excel workbook for easy sharing with non-technical stakeholders.
* **Reproducible Logs**: A log file for each stage of the pipeline for easy debugging and auditing.

---

## üèóÔ∏è Architecture

The pipeline follows a straightforward, multi-stage architecture designed for clarity and scalability. Raw data flows from a CSV source, is processed in-memory by Spark, stored in an efficient format (Parquet), and then consumed by downstream analytics and visualization layers in both Python and R.



**Data Flow:**
`CSV` ‚û°Ô∏è `Spark DataFrame` ‚û°Ô∏è `Cleaned Parquet` ‚û°Ô∏è `SQL/Pandas Analytics` ‚û°Ô∏è `CSV / Excel / PNGs`
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚Üò `R (arrow + tidyverse)` ‚û°Ô∏è `PNGs`

* **Storage Layer**: **Parquet** is used for the cleaned data, offering excellent compression and fast columnar read performance, which is ideal for analytics.
* **Processing Engine**: **Apache Spark** serves as the core engine, providing scalable data processing capabilities that work just as well on a local machine as on a multi-node cluster.
* **Presentation Layer**: **Python (matplotlib)** and **R (ggplot2)** are used to create ready-to-share visualizations for reports and presentations.

---

## üìÇ Folder Structure

The project is organized to separate raw data, outputs, source code, and reports, making it easy to navigate and maintain.

```
Sales_Analyzer/
‚îú‚îÄ data/
‚îÇ  ‚îî‚îÄ online_retail_II.csv          # Raw input data
‚îú‚îÄ spark_output/
‚îÇ  ‚îî‚îÄ clean_online_retail_parquet/  # Cleaned dataset (Parquet directory)
‚îú‚îÄ python_reports/                  # CSVs & other analytics outputs from Python
‚îú‚îÄ R_reports/                       # Outputs produced by the R script
‚îú‚îÄ excel_output/                    # Final Excel workbook for business users
‚îú‚îÄ docs/                            # Final PNG images for documentation
‚îú‚îÄ logs/                            # Log file for each pipeline stage
‚îú‚îÄ notebooks/                       # All the Python and R scripts for the pipeline
‚îÇ  ‚îú‚îÄ 00_start_spark.py
‚îÇ  ‚îú‚îÄ 01_ingest_and_preview.py
‚îÇ  ‚îú‚îÄ 02_clean.py
‚îÇ  ‚îú‚îÄ 03_queries.py
‚îÇ  ‚îú‚îÄ 04_plots.py
‚îÇ  ‚îî‚îÄ 05_visualize.R
‚îú‚îÄ tmp/                             # Spark scratch space for temporary files
‚îî‚îÄ run_all.ps1                      # One-click script to run the entire pipeline
```

---

## üõ†Ô∏è Tech Stack & Rationale

| Technology             | Version                | Purpose                                                              |
| ---------------------- | ---------------------- | -------------------------------------------------------------------- |
| **Apache Spark** | 4.0.0 (Hadoop 3)       | Scalable ETL and SQL engine. Code runs locally or on a cluster.      |
| **Python** | 3.11                   | Primary language for orchestration, data cleaning, and analytics.    |
| **PySpark** | 4.0.0                  | The Python API for Apache Spark.                                     |
| **Parquet** | -                      | Efficient columnar storage format ideal for big data analytics.      |
| **Matplotlib** | 3.8.4                  | Stable and lightweight plotting library, friendly for Windows setups.  |
| **R** | 4.5.1                  | Alternative analytics surface, showcasing language interoperability. |
| **tidyverse & arrow** | -                      | For modern data manipulation (dplyr) and Parquet access (arrow) in R. |
| **PowerShell** | -                      | Simple and readable orchestration script for Windows.                |
| **xlsxwriter** | -                      | Python library to create business-friendly Excel reports.            |

---

## ‚úÖ Prerequisites (Windows)

Before running the pipeline, ensure the following are installed and configured on your Windows machine.

1.  **Java (JDK 17+)**: Spark runs on the JVM. Verify your installation by running `java -version` in PowerShell.
2.  **Apache Spark 4.0.0**:
    * Download Spark 4.0.0 pre-built for Hadoop 3.
    * Unzip it to a memorable location (e.g., `C:\spark`).
    * Add the `bin` directory (e.g., `C:\spark\bin`) to your system's `PATH` environment variable.
3.  **Windows Hadoop Helpers (`winutils.exe`)**:
    * Spark requires `winutils.exe` and `hadoop.dll` to work correctly on Windows. You can get these from a reputable source (like the [winutils GitHub repo](https://github.com/cdarlint/winutils)) and place them directly inside your Spark `bin` folder (e.g., `C:\spark\bin`).
4.  **R 4.5.1 (Optional)**:
    * Required only for `Stage 5`.
    * Install R and ensure its `bin` directory is on your `PATH` so that `Rscript` is a recognized command in PowerShell.
5.  **Python 3.11 Dependencies**:
    * It's highly recommended to use a virtual environment.
    ```powershell
    # Create and activate a virtual environment
    python -m venv venv
    .\venv\Scripts\Activate.ps1

    # Install the required packages
    pip install pyspark==4.0.0 pandas "numpy<2" matplotlib==3.8.4 xlsxwriter
    ```
    > **Note:** We pin `numpy<2` and `matplotlib<3.8.4` to avoid known binary compatibility issues that can arise on Windows.

---

## üöÄ How to Run

### Run the Entire Pipeline

Execute one simple command in your PowerShell terminal from the project root directory:

```powershell
./run_all.ps1
```

This will run all five stages in sequence, generating logs and outputs in their respective folders.

### Run a Single Stage

You can also run any script individually for debugging or development. For example, to run only the cleaning stage:

```powershell
python notebooks/02_clean.py
```

---

## üî¢ The Pipeline ‚Äì Stage by Stage

### `00_start_spark.py` (Optional Sanity Check)

* **Purpose**: A simple script to confirm that your Spark, Java, and Python environment is correctly configured.
* **Logic**: It initializes a `SparkSession`, prints the Spark version, creates a tiny DataFrame, and displays it. This is the first thing to run if you encounter environment issues.
* **Output**: A success message and DataFrame printed to the console.

### `01_ingest_and_preview.py`

* **Purpose**: To read the raw CSV data, infer its schema, and log a preview.
* **Logic**:
    * Uses `spark.read.csv()` with `header` and `inferSchema` options set to `true`.
    * Prints the inferred schema to the console and log file. This helps catch potential data type issues early (e.g., a numeric column being read as a string).
    * Shows the first few rows of the raw data.
* **Output**: A log file (`logs/01_ingest.log`) containing the raw schema and a data sample.

### `02_clean.py`

* **Purpose**: To transform the raw data into a clean, structured dataset ready for analysis and save it as Parquet.
* **Key Transformations**:
    1.  **Trimming Whitespace**: Removes leading/trailing spaces from the `Description` field.
    2.  **Type Casting**: Ensures `Quantity` and `Price` are `double` types for accurate calculations.
    3.  **Filtering Records**:
        * Keeps only rows where `Quantity > 0`.
        * Excludes returns/credits by filtering out invoices that start with `"C"`.
        * Drops rows with a null `Customer ID` to ensure the quality of customer-level analytics.
    4.  **Creating Derived Fields**:
        * `Revenue` is calculated as `Quantity * Price`.
        * Time dimensions (`year`, `month`, `day`, `hour`) are extracted from `InvoiceDate` for easy time-based aggregation.
* **Output**: A partitioned Parquet dataset written to `spark_output/clean_online_retail_parquet/`.

### `03_queries.py`

* **Purpose**: To run foundational business queries on the clean data and export the results as CSV files.
* **Queries**:
    1.  **Monthly Revenue**: Aggregates total revenue by year and month.
    2.  **Top 10 Products**: Ranks products by total revenue to identify best-sellers.
    3.  **Top 10 Customers**: Ranks customers by total revenue to identify high-value accounts.
* **Output**: Three separate CSV files located in `python_reports/monthly_revenue/`, `python_reports/top_products/`, and `python_reports/top_customers/`.

### `04_plots.py`

* **Purpose**: To create a simple visualization from the analytics results using Python.
* **Logic**:
    * Reads the monthly revenue CSV generated in the previous stage into a Pandas DataFrame.
    * Uses `matplotlib` to create a line chart visualizing revenue trends over time.
* **Output**: A PNG image saved to `docs/monthly_revenue.png`.

### `05_visualize.R`

* **Purpose**: To demonstrate how the Parquet data can be consumed by another language (R) for analysis and visualization.
* **Logic**:
    * Uses the `arrow` package to efficiently read the partitioned Parquet dataset directly into R.
    * Leverages `dplyr` for data manipulation to calculate monthly revenue and top products.
    * Uses `ggplot2` to create high-quality visualizations.
* **Output**: Two PNG images saved to `docs/monthly_revenue_R.png` and `docs/top_products_R.png`.

---

## üìä Outputs & How to Interpret Them

| Output                           | Location                                           | Interpretation & Action                                                                                                                             |
| -------------------------------- | -------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Clean Parquet Data** | `spark_output/clean_online_retail_parquet/`        | This is your **single source of truth**. Connect it to BI tools like Power BI/Tableau or use it for further ad-hoc analysis.                |
| **Monthly Revenue (CSV/PNG)** | `python_reports/monthly_revenue/`, `docs/`         | **Ask**: Are there seasonal trends? Which months are strongest? **Act**: Plan marketing campaigns around peak months; investigate dips.     |
| **Top Products (CSV/R PNG)** | `python_reports/top_products/`, `docs/`            | **Ask**: Does the 80/20 rule apply? Are a few products driving most sales? **Act**: Prioritize inventory and marketing for top SKUs.           |
| **Top Customers (CSV)** | `python_reports/top_customers/`                    | **Ask**: Is there a high concentration of revenue from a few customers? **Act**: Develop retention strategies for high-value customers.      |
| **Excel Workbook (Optional)** | `excel_output/Sales_Report.xlsx`                   | **Ask**: How can I share this with my manager? **Act**: Email this workbook. It contains key metrics in separate sheets for easy digestion. |

---

## üêõ Troubleshooting (Windows-Specific)

* **`UnicodeEncodeError` / `cp1252`**: This pipeline avoids printing special characters to the console to prevent this common Windows terminal error.
* **`UnsatisfiedLinkError` (NativeIO)**: This usually means `winutils.exe` or `hadoop.dll` is missing. Ensure both files are in your `spark\bin` directory and restart your terminal.
* **Spark Temp Dir Warning**: The scripts explicitly set `SPARK_LOCAL_DIRS` to a path within the project to prevent warnings about default Windows temp directories.
* **`Rscript` not recognized**: Your R installation's `bin` folder is not on your system `PATH`. Add it and restart your terminal/VS Code.
* **R `arrow` Access Denied**: Another program (like Windows Explorer) may have a lock on the Parquet files. Close any open file explorers pointing to the `spark_output` directory. Use `arrow::open_dataset()` as it's designed to read directories, not single files.

---

## üí° Extending the Project

This pipeline provides a solid foundation. Here are some ideas for extending it:

* **Add More KPIs**: Calculate Average Order Value (AOV), basket size, or net revenue (sales minus returns).
* **Cohort Analysis**: Group customers by their first purchase month to track retention over time.
* **Forecasting**: Use libraries like `Prophet` or `statsmodels` to forecast future monthly revenue.
* **Dashboarding**: Connect a BI tool like Power BI or build a simple Streamlit app on top of the clean Parquet data.
* **Configuration**: Move file paths and parameters into a `config.yaml` file to make the pipeline more configurable.

---

## üìú License

This project is licensed under the MIT License. See the `LICENSE` file for details.
