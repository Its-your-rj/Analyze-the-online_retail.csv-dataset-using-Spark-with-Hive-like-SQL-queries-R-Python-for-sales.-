# Sales_Analyzer: End-to-End Retail Analytics Pipeline üìà

This repository contains a reproducible, end-to-end data pipeline that ingests raw retail transaction data, cleans and models it with **Apache Spark**, runs analytics queries, and produces business-ready outputs like **CSVs, PNG plots, and an Excel workbook**. The entire pipeline is orchestrated with PowerShell for a one-click run on **Windows** and is designed to run completely on your local machine.

---

## üéØ Goals & Deliverables

The primary objective of this project is to provide a clean, production-style template for retail analytics using Apache Spark. It demonstrates a full data lifecycle, from a raw CSV file to actionable business artifacts, executed with a single command.

**What you get when you run the pipeline:**

* **Cleaned Parquet Dataset**: A columnar, compressed, and analytics-friendly dataset ready for BI tools or further analysis.
* **KPI Exports (CSV)**: Key metrics like monthly revenue, top customers, and top products.
* **Visualizations (PNG)**: Charts generated by both Python (Matplotlib) and R (ggplot2).
* **Business-Ready Report (Excel)**: An optional multi-sheet Excel workbook for easy sharing with non-technical stakeholders.
* **Reproducible Logs**: A log file for each stage of the pipeline for easy debugging and auditing.

---

## üèóÔ∏è Architecture

The pipeline follows a straightforward, multi-stage architecture designed for clarity and scalability. Raw data flows from a CSV source, is processed in-memory by Spark, stored in an efficient format (Parquet), and then consumed by downstream analytics and visualization layers in both Python and R.



**Data Flow:**
`CSV` ‚û°Ô∏è `Spark DataFrame` ‚û°Ô∏è `Cleaned Parquet` ‚û°Ô∏è `SQL/Pandas Analytics` ‚û°Ô∏è `CSV / Excel / PNGs`
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚Üò `R (arrow + tidyverse)` ‚û°Ô∏è `PNGs`

* **Storage Layer**: **Parquet** is used for the cleaned data, offering excellent compression and fast columnar read performance, which is ideal for analytics.
* **Processing Engine**: **Apache Spark** serves as the core engine, providing scalable data processing capabilities that work just as well on a local machine as on a multi-node cluster.
* **Presentation Layer**: **Python (matplotlib)** and **R (ggplot2)** are used to create ready-to-share visualizations for reports and presentations.

---

## üìÇ Folder Structure

The project is organized to separate raw data, outputs, source code, and reports, making it easy to navigate and maintain.

```
Sales_Analyzer/
‚îú‚îÄ data/
‚îÇ  ‚îî‚îÄ online_retail_II.csv          # Raw input data
‚îú‚îÄ spark_output/
‚îÇ  ‚îî‚îÄ clean_online_retail_parquet/  # Cleaned dataset (Parquet directory)
‚îú‚îÄ python_reports/                  # CSVs & other analytics outputs from Python
‚îú‚îÄ R_reports/                       # Outputs produced by the R script
‚îú‚îÄ excel_output/                    # Final Excel workbook for business users
‚îú‚îÄ docs/                            # Final PNG images for documentation
‚îú‚îÄ logs/                            # Log file for each pipeline stage
‚îú‚îÄ notebooks/                       # All the Python and R scripts for the pipeline
‚îÇ  ‚îú‚îÄ 00_start_spark.py
‚îÇ  ‚îú‚îÄ 01_ingest_and_preview.py
‚îÇ  ‚îú‚îÄ 02_clean.py
‚îÇ  ‚îú‚îÄ 03_queries.py
‚îÇ  ‚îú‚îÄ 04_plots.py
‚îÇ  ‚îî‚îÄ 05_visualize.R
‚îú‚îÄ tmp/                             # Spark scratch space for temporary files
‚îî‚îÄ run_all.ps1                      # One-click script to run the entire pipeline
```

---

## üõ†Ô∏è Tech Stack & Rationale

| Technology             | Version                | Purpose                                                              |
| ---------------------- | ---------------------- | -------------------------------------------------------------------- |
| **Apache Spark** | 4.0.0 (Hadoop 3)       | Scalable ETL and SQL engine. Code runs locally or on a cluster.      |
| **Python** | 3.11                   | Primary language for orchestration, data cleaning, and analytics.    |
| **PySpark** | 4.0.0                  | The Python API for Apache Spark.                                     |
| **Parquet** | -                      | Efficient columnar storage format ideal for big data analytics.      |
| **Matplotlib** | 3.8.4                  | Stable and lightweight plotting library, friendly for Windows setups.  |
| **R** | 4.5.1                  | Alternative analytics surface, showcasing language interoperability. |
| **tidyverse & arrow** | -                      | For modern data manipulation (dplyr) and Parquet access (arrow) in R. |
| **PowerShell** | -                      | Simple and readable orchestration script for Windows.                |
| **xlsxwriter** | -                      | Python library to create business-friendly Excel reports.            |

---

## ‚úÖ Prerequisites (Windows)

Before running the pipeline, ensure the following are installed and configured on your Windows machine.

1.  **Java (JDK 17+)**: Spark runs on the JVM. Verify your installation by running `java -version` in PowerShell.
2.  **Apache Spark 4.0.0**:
    * Download Spark 4.0.0 pre-built for Hadoop 3.
    * Unzip it to a memorable location (e.g., `C:\spark`).
    * Add the `bin` directory (e.g., `C:\spark\bin`) to your system's `PATH` environment variable.
3.  **Windows Hadoop Helpers (`winutils.exe`)**:
    * Spark requires `winutils.exe` and `hadoop.dll` to work correctly on Windows. You can get these from a reputable source (like the [winutils GitHub repo](https://github.com/cdarlint/winutils)) and place them directly inside your Spark `bin` folder (e.g., `C:\spark\bin`).
4.  **R 4.5.1 (Optional)**:
    * Required only for `Stage 5`.
    * Install R and ensure its `bin` directory is on your `PATH` so that `Rscript` is a recognized command in PowerShell.
5.  **Python 3.11 Dependencies**:
    * It's highly recommended to use a virtual environment.
    ```powershell
    # Create and activate a virtual environment
    python -m venv venv
    .\venv\Scripts\Activate.ps1

    # Install the required packages
    pip install pyspark==4.0.0 pandas "numpy<2" matplotlib==3.8.4 xlsxwriter
    ```
    > **Note:** We pin `numpy<2` and `matplotlib<3.8.4` to avoid known binary compatibility issues that can arise on Windows.

---

## üöÄ How to Run

### Run the Entire Pipeline

Execute one simple command in your PowerShell terminal from the project root directory:

```powershell
./run_all.ps1
```

This will run all five stages in sequence, generating logs and outputs in their respective folders.

### Run a Single Stage

You can also run any script individually for debugging or development. For example, to run only the cleaning stage:

```powershell
python notebooks/02_clean.py
```

---

## üî¢ The Pipeline ‚Äì Stage by Stage

Here is a detailed breakdown of each stage in the analytics pipeline.

### Stage 00: Sanity Check (`00_start_spark.py`) üõ†Ô∏è

* **Objective**: To perform a basic "smoke test" of the environment before running any real workload.
* **The "Why"**: Data pipelines involving Spark have several dependencies (Java, Python, Spark, environment variables). A failure in any of these can cause cryptic errors later. This script isolates the environment setup from the data processing logic. If this script fails, the problem is with the installation or configuration, not the pipeline code.
* **Step-by-Step Logic Breakdown**:
    1.  **Import `SparkSession`**: This is the entry point to all Spark functionality.
    2.  **Initialize `SparkSession`**: The code `SparkSession.builder.appName("SanityCheck").getOrCreate()` tells Spark to find or create a new Spark session for this application.
    3.  **Create a DataFrame**: It creates a tiny, hardcoded dataset and converts it into a Spark DataFrame, confirming that Spark's core data structure is working.
    4.  **Show the DataFrame**: Calling `.show()` triggers an action in Spark, forcing it to execute the plan and display the result, which confirms the execution engine is operational.
* **Expected Outcome**: You see the Spark version and a small table printed to your console. This is a definitive confirmation that your environment is ready.

### Stage 01: Ingest and Preview (`01_ingest_and_preview.py`) üìù

* **Objective**: To load the raw data from its source (CSV) into Spark and perform an initial inspection.
* **The "Why"**: Before cleaning data, you must understand its initial state. This stage acts as a documented first look. By inferring the schema, you let Spark guess the data types. Logging this is crucial for reproducibility, as it records exactly how Spark first interpreted the raw file.
* **Step-by-Step Logic Breakdown**:
    1.  **Read CSV**: `spark.read.option("header","true").csv(...)` tells Spark to use the first row of the CSV as column names.
    2.  **Infer Schema**: `option("inferSchema","true")` instructs Spark to scan the data to guess the data type for each column. This is convenient for exploration, though in production an explicit schema is more reliable.
    3.  **Print Schema**: The `.printSchema()` method displays the column names and their inferred data types, helping you spot potential issues (e.g., a `Price` column read as a string).
    4.  **Show Data**: `.show()` displays the top rows of the raw DataFrame, giving you a tangible preview of the data.
* **Expected Outcome**: A log file (`01_ingest.log`) is created, serving as a permanent record of the raw data's structure at the time of ingestion.

### Stage 02: Clean and Transform (`02_clean.py`) üßπ

* **Objective**: To apply business rules and transformations to convert the raw, messy data into a clean, reliable, and analytics-ready dataset.
* **The "Why"**: This is the most critical stage of the ETL process. Raw data often contains errors, irrelevant information (like returns), and missing values. This stage enforces data quality and shapes the data according to business definitions. Writing the output to **Parquet** is a deliberate choice for efficiency; its columnar format dramatically speeds up analytical queries compared to row-based formats like CSV.
* **Step-by-Step Logic Breakdown**:
    1.  **Read Raw Data**: The script starts by loading the raw data ingested in the previous stage.
    2.  **Apply Transformations**:
        * `trim(Description)`: Removes leading/trailing spaces from product descriptions to ensure consistent grouping.
        * **Type Casting**: Explicitly converts `Quantity` and `Price` to a numeric type (e.g., `Double`) to enable mathematical operations.
        * **Filter `Quantity > 0`**: Removes data errors or stock adjustments to focus analysis on actual sales.
        * **Filter `~col("Invoice").startswith("C")`**: Excludes returns and cancellations (a key business rule) to create a dataset of **gross sales**.
        * **Filter `col("Customer ID").isNotNull()`**: Removes anonymous purchases to ensure the quality of customer-centric analytics.
        * **Create `Revenue`**: Engineers a new feature (`Quantity * Price`), which is the primary metric for most sales analyses.
        * **Create Time Dimensions**: Extracts `year`, `month`, `day`, and `hour` from `InvoiceDate` to make time-based aggregations highly efficient.
    3.  **Write to Parquet**: The final, cleaned DataFrame is written to a directory using `.write.mode("overwrite").parquet(...)`. `overwrite` ensures the data is fresh on each pipeline run.
* **Expected Outcome**: A directory named `clean_online_retail_parquet` containing compressed Parquet files. This directory is now the **single source of truth** for all subsequent analytics.

### Stage 03: Run Analytics Queries (`03_queries.py`) üìä

* **Objective**: To answer fundamental business questions by querying the clean dataset and saving the results into a shareable CSV format.
* **The "Why"**: While Parquet is great for machines, business users often need simple, aggregated summaries. This stage bridges that gap by generating high-level Key Performance Indicators (KPIs) and making them accessible in a universally used format.
* **Step-by-Step Logic Breakdown**:
    1.  **Read Clean Data**: The script reads the clean Parquet dataset from the `spark_output/` directory.
    2.  **Run Queries**: It uses Spark SQL or the DataFrame API to perform aggregations:
        * **Monthly Revenue**: `GROUP BY year, month` and `SUM(Revenue)` to track sales trends.
        * **Top 10 Products**: `GROUP BY Description`, `SUM(Revenue)`, then `ORDER BY total_revenue DESC` to identify best-sellers.
        * **Top 10 Customers**: Applies the same logic to `Customer ID` to find high-value accounts.
    3.  **Write to CSV**: Each query result is written to its own folder as a CSV file.
* **Expected Outcome**: Three folders inside `python_reports/`, each containing CSV files with aggregated results for monthly revenue, top products, and top customers.

### Stage 04: Visualize with Python (`04_plots.py`) üìà

* **Objective**: To visually represent one of the key findings from the query stage using Python.
* **The "Why"**: Visualizations make trends and patterns instantly obvious. A line chart of monthly revenue is far more intuitive for spotting seasonality or growth than a table of numbers. This stage creates a report-ready PNG chart.
* **Step-by-Step Logic Breakdown**:
    1.  **Read Aggregated Data**: This lightweight script reads the small `monthly_revenue.csv` into a **Pandas DataFrame**, a common pattern where Spark does the heavy lifting and Pandas handles final, small-scale manipulation.
    2.  **Plotting**: It uses the `matplotlib` library to create a line chart with clear labels and a title.
    3.  **Save Figure**: The generated plot is saved as a PNG file to the `docs/` folder.
* **Expected Outcome**: A `monthly_revenue.png` image file in the `docs/` folder, ready to be embedded in a presentation or report.

### Stage 05: Visualize with R (`05_visualize.R`) üé®

* **Objective**: To demonstrate the language-agnostic nature of the Parquet data store by recreating the analysis in R.
* **The "Why"**: Different teams prefer different tools. This stage proves that the Parquet output from Stage 02 is a universal asset that can be consumed by multiple downstream tools (like R) without conversion, fostering collaboration between data engineering and data science teams.
* **Step-by-Step Logic Breakdown**:
    1.  **Read Parquet with `arrow`**: The R script uses `arrow::open_dataset()` to efficiently read the partitioned Parquet files without loading the entire dataset into memory, making it incredibly fast.
    2.  **Data Manipulation with `dplyr`**: It uses `dplyr` verbs (`group_by`, `summarise`) to perform the same aggregations for monthly revenue and top products.
    3.  **Visualization with `ggplot2`**: It uses `ggplot2`, the premier data visualization library in R, to create high-quality, aesthetically pleasing plots.
* **Expected Outcome**: Two new PNG files in the `docs/` folder (`monthly_revenue_R.png` and `top_products_R.png`), showcasing the results from an R-based analysis.

---

## üìä Outputs & How to Interpret Them

| Output                           | Location                                           | Interpretation & Action                                                                                                                             |
| -------------------------------- | -------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Clean Parquet Data** | `spark_output/clean_online_retail_parquet/`        | This is your **single source of truth**. Connect it to BI tools like Power BI/Tableau or use it for further ad-hoc analysis.                |
| **Monthly Revenue (CSV/PNG)** | `python_reports/monthly_revenue/`, `docs/`         | **Ask**: Are there seasonal trends? Which months are strongest? **Act**: Plan marketing campaigns around peak months; investigate dips.     |
| **Top Products (CSV/R PNG)** | `python_reports/top_products/`, `docs/`            | **Ask**: Does the 80/20 rule apply? Are a few products driving most sales? **Act**: Prioritize inventory and marketing for top SKUs.           |
| **Top Customers (CSV)** | `python_reports/top_customers/`                    | **Ask**: Is there a high concentration of revenue from a few customers? **Act**: Develop retention strategies for high-value customers.      |
| **Excel Workbook (Optional)** | `excel_output/Sales_Report.xlsx`                   | **Ask**: How can I share this with my manager? **Act**: Email this workbook. It contains key metrics in separate sheets for easy digestion. |

---

## üêõ Troubleshooting (Windows-Specific)

* **`UnicodeEncodeError` / `cp1252`**: This pipeline avoids printing special characters to the console to prevent this common Windows terminal error.
* **`UnsatisfiedLinkError` (NativeIO)**: This usually means `winutils.exe` or `hadoop.dll` is missing. Ensure both files are in your `spark\bin` directory and restart your terminal.
* **Spark Temp Dir Warning**: The scripts explicitly set `SPARK_LOCAL_DIRS` to a path within the project to prevent warnings about default Windows temp directories.
* **`Rscript` not recognized**: Your R installation's `bin` folder is not on your system `PATH`. Add it and restart your terminal/VS Code.
* **R `arrow` Access Denied**: Another program (like Windows Explorer) may have a lock on the Parquet files. Close any open file explorers pointing to the `spark_output` directory. Use `arrow::open_dataset()` as it's designed to read directories, not single files.

---

## üí° Extending the Project

This pipeline provides a solid foundation. Here are some ideas for extending it:

* **Add More KPIs**: Calculate Average Order Value (AOV), basket size, or net revenue (sales minus returns).
* **Cohort Analysis**: Group customers by their first purchase month to track retention over time.
* **Forecasting**: Use libraries like `Prophet` or `statsmodels` to forecast future monthly revenue.
* **Dashboarding**: Connect a BI tool like Power BI or build a simple Streamlit app on top of the clean Parquet data.
* **Configuration**: Move file paths and parameters into a `config.yaml` file to make the pipeline more configurable.

---

## üìú License

This project is licensed under the MIT License. See the `LICENSE` file for details.
